{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World of Supply\n",
    "\n",
    "A simulation environment for multi-echelon supply chain optimization problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm as tqdm\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Simulation Logic and Rendering\n",
    "\n",
    "In this section, we test the core simulator and renderer (without RL adapters and integrations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import world_of_supply_environment as ws\n",
    "importlib.reload(ws)\n",
    "import world_of_supply_renderer as wsr\n",
    "importlib.reload(wsr)\n",
    "\n",
    "# Measure the simulation rate, steps/sec\n",
    "world = ws.WorldBuilder.create(80, 16)\n",
    "policy = ws.SimpleControlPolicy()\n",
    "for i in tqdm(range(10000)):\n",
    "    world.act(policy.compute_control(world))\n",
    "    \n",
    "# Test rendering\n",
    "renderer = wsr.AsciiWorldRenderer()\n",
    "frame_seq = []\n",
    "world = ws.WorldBuilder.create(80, 16)\n",
    "policy = ws.SimpleControlPolicy()\n",
    "for epoch in tqdm(range(300)):\n",
    "    frame = renderer.render(world)\n",
    "    frame_seq.append(np.asarray(frame))\n",
    "    world.act(policy.compute_control(world))\n",
    "\n",
    "print('Rendering the animation...')\n",
    "wsr.AsciiWorldRenderer.plot_sequence_images(frame_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Training\n",
    "\n",
    "In this section, we run RLlib policy trainers. These trainers evaluate the hand coded policy, learn a new policy from scrath, or learn a new policy by playing against the hand coded policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import world_of_supply_rllib as wsr\n",
    "importlib.reload(wsr)\n",
    "import world_of_supply_rllib_training as wst\n",
    "importlib.reload(wst)\n",
    "\n",
    "# Policy training\n",
    "#trainer = wst.play_baseline(n_iterations = 2)\n",
    "trainer = wst.train_ppo(n_iterations = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "In this section, we evaluate the trained policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering One Episod for the Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import world_of_supply_renderer as wsren\n",
    "importlib.reload(wsren)\n",
    "import world_of_supply_rllib as wsrl\n",
    "importlib.reload(wsrl)\n",
    "import world_of_supply_rllib_training as wstr\n",
    "importlib.reload(wstr)\n",
    "\n",
    "# Parameters of the tracing simulation\n",
    "episod_duration = 1000\n",
    "steps_to_render = (0, episod_duration)\n",
    "\n",
    "# Create the environment\n",
    "renderer = wsren.AsciiWorldRenderer()\n",
    "frame_seq = []\n",
    "env = wsrl.WorldOfSupplyEnv(wstr.env_config)\n",
    "states = env.reset()\n",
    "infos = None\n",
    "    \n",
    "def load_policy(facility_id):\n",
    "    #return wsrl.SimplePolicy(env.observation_space, env.action_space, wsrl.SimplePolicy.get_config_from_env(env))\n",
    "    \n",
    "    policy_map = wstr.policy_mapping_global.copy()\n",
    "    #policy_map['LumberFactory'] = 'ppo'\n",
    "    return trainer.get_policy(wstr.create_policy_mapping_fn(policy_map)(facility_id))\n",
    "\n",
    "policies = {}\n",
    "rnn_states = {}\n",
    "for facility_id in states.keys():\n",
    "    policies[facility_id] = load_policy(facility_id)\n",
    "    rnn_states[facility_id] = policies[facility_id].get_initial_state()\n",
    "    \n",
    "# Simulation loop\n",
    "for epoch in tqdm(range(episod_duration)):        \n",
    "    action_dict = {}\n",
    "    for facility_id, state in states.items():\n",
    "        \n",
    "        policy = policies[facility_id]\n",
    "        rnn_state = rnn_states[facility_id]\n",
    "        \n",
    "        if infos is not None and facility_id in infos:\n",
    "            action_dict[facility_id], rnn_state, _ = policy.compute_single_action( state, info=infos[facility_id], state=rnn_state ) \n",
    "        else:\n",
    "            action_dict[facility_id], rnn_state, _ = policy.compute_single_action( state, state=rnn_state )\n",
    "        \n",
    "    states, reward, dones, infos = env.step(action_dict)\n",
    "    \n",
    "    if epoch >= steps_to_render[0] and epoch < steps_to_render[1]:\n",
    "        frame = renderer.render(env.world)\n",
    "        frame_seq.append(np.asarray(frame))\n",
    "        \n",
    "print('Rendering the animation...')\n",
    "wsren.AsciiWorldRenderer.plot_sequence_images(frame_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
