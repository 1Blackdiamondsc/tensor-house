{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models from Introduction to Algorithmic Marketing\n",
    "# https://algorithmicweb.wordpress.com/\n",
    "#\n",
    "# Latent Semantic Analysis (LSA) is a method of text analysis \n",
    "# that helps to identify concepts represented in the text as related words\n",
    "# In this example, we both calculate document representation in the \n",
    "# concept space and score documents againt the query using a distance metric in \n",
    "# this space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sympy as sy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"chicago chocolate retro candies made with love\",\n",
    "\"chocolate sweets and candies collection with mini love hearts\",\n",
    "\"retro sweets from chicago for chocolate lovers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic analyzer: \n",
    "# - split documents into words\n",
    "# - remove stop words\n",
    "# - apply a simple stemmer\n",
    "analyzer = {\n",
    "    \"with\": None,\n",
    "    \"for\": None,\n",
    "    \"and\": None,\n",
    "    \"from\": None,\n",
    "    \"lovers\": \"love\",\n",
    "    \"hearts\": \"heart\"\n",
    "}\n",
    "bag_of_words_docs = [list(filter(None, [analyzer.get(word, word) for word in d.split()])) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     heart [0 1 0]\n",
      "   candies [1 1 0]\n",
      "collection [0 1 0]\n",
      "      mini [0 1 0]\n",
      "      love [1 1 1]\n",
      "      made [1 0 0]\n",
      "    sweets [0 1 1]\n",
      "     retro [1 0 1]\n",
      "   chicago [1 0 1]\n",
      " chocolate [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Create term frequency matrix\n",
    "unique_words = list(set(chain.from_iterable(bag_of_words)))\n",
    "word_freq = [Counter(d) for d in bag_of_words_docs]\n",
    "A = np.array([[freq.get(word, 0) for freq in word_freq] for word in unique_words])\n",
    "for i, word in enumerate(unique_words):\n",
    "    print(\"%10s %s\" % (word, str(A[i])))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16776201 -0.40589961]\n",
      " [-0.33388781 -0.14894709]\n",
      " [-0.16776201 -0.40589961]\n",
      " [-0.16776201 -0.40589961]\n",
      " [-0.4857994   0.0183087 ]\n",
      " [-0.1661258   0.25695253]\n",
      " [-0.31967359 -0.23864383]\n",
      " [-0.31803739  0.42420831]\n",
      " [-0.31803739  0.42420831]\n",
      " [-0.4857994   0.0183087 ]]\n",
      "[[ 3.56192303  0.        ]\n",
      " [ 0.          1.96587909]]\n",
      "[[-0.59172732 -0.59755537 -0.54109736]\n",
      " [ 0.5051376  -0.79794957  0.32880465]]\n",
      "[[-0.  1.  0.]\n",
      " [ 1.  1.  1.]\n",
      " [-0.  1.  0.]\n",
      " [-0.  1.  0.]\n",
      " [ 1.  1.  1.]\n",
      " [ 1. -0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  1.]\n",
      " [ 1.  0.  1.]\n",
      " [ 1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Perform truncated SVD decomposition \n",
    "U, s, V = np.linalg.svd(A, full_matrices=False)\n",
    "truncate_rank = 2\n",
    "Ut = U[:, 0:truncate_rank]\n",
    "Vt = V[0:truncate_rank, :]\n",
    "St = np.diag(s[0:truncate_rank])\n",
    "reconstruction = np.dot(Ut, np.dot(St, Vt))\n",
    "print(Ut)\n",
    "print(St)\n",
    "print(Vt)\n",
    "print(np.round(reconstruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 score: 0.890730150933\n",
      "Document 1 score: -0.51043666768\n",
      "Document 2 score: 0.806592806364\n"
     ]
    }
   ],
   "source": [
    "# Project a query to the concept space and score documents\n",
    "query = \"chicago\"\n",
    "q = [int(query == word) for word in unique_words]\n",
    "qs = np.dot(q, np.dot(Ut, np.linalg.inv(St)))\n",
    "\n",
    "def score(query_vec, doc_vec):\n",
    "    return np.dot(query_vec, doc_vec) / ( np.linalg.norm(query_vec) * np.linalg.norm(doc_vec) )\n",
    "\n",
    "for d in range(len(docs)):\n",
    "    print(\"Document %s score: %s\" % (d, score(qs, Vt[:, d])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
